{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 07:35:28.954932: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-30 07:35:29.058616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738211729.099639   18074 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738211729.111032   18074 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-30 07:35:29.203450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.360912322998047"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> sleep in <extra_id_1> crib\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute baby <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>the Straw Hat Pirates befriends a pirate hunter and swordsman named Roronoa Zoro. they are joined in their journey by Nami, a money-obsessed thief and navigator. others join the crew later in the series, including Tony Tony Chopper, an anthropomorphized reinde\n"
     ]
    }
   ],
   "source": [
    "## TEXT SUMMARIZATION\n",
    "\n",
    "#example from https://wandb.ai/mukilan/T5_transformer/reports/Exploring-Google-s-T5-Text-To-Text-Transformer-Model--VmlldzoyNjkzOTE2\n",
    "\n",
    "one_piece_sequence = (\"The series focuses on Monkey D. Luffy, a young man made of rubber, who, inspired by his childhood idol,\"\n",
    "\"the powerful pirate Red-Haired Shanks, sets off on a journey from the East Blue Sea to find the mythical treasure,\"\n",
    "\"the One Piece, and proclaim himself the King of the Pirates. In an effort to organize his own crew, the Straw Hat Pirates,\"\n",
    "\"Luffy rescues and befriends a pirate hunter and swordsman named Roronoa Zoro, and they head off in search of the \"\n",
    "\"titular treasure. They are joined in their journey by Nami, a money-obsessed thief and navigator; Usopp, a sniper \"\n",
    "\"and compulsive liar; and Sanji, a perverted but chivalrous cook. They acquire a ship, the Going Merry, and engage in confrontations\"\n",
    "\"with notorious pirates of the East Blue. As Luffy and his crew set out on their adventures, others join the crew later in the series, \"\n",
    "\"including Tony Tony Chopper, an anthropomorphized reindeer doctor; Nico Robin, an archaeologist and former Baroque Works assassin; \"\n",
    "\"Franky, a cyborg shipwright; Brook, a skeleton musician and swordsman; and Jimbei, a fish-man helmsman and former member of the Seven \"\n",
    "\"Warlords of the Sea. Once the Going Merry is damaged beyond repair, Franky builds the Straw Hat Pirates a new ship, the Thousand Sunny,\"\n",
    "\"Together, they encounter other pirates, bounty hunters, criminal organizations, revolutionaries, secret agents, and soldiers of the\"\n",
    "\"corrupt World Government, and various other friends and foes, as they sail the seas in pursuit of their dreams.\")\n",
    "inputs = tokenizer.encode(\"summarize: \" + one_piece_sequence,\n",
    "return_tensors='pt',\n",
    "max_length=512,\n",
    "truncation=True)\n",
    "summarization_ids = model.generate(inputs, max_length=80, min_length=40, length_penalty=5., num_beams=2)\n",
    "summarization = tokenizer.decode(summarization_ids[0])\n",
    "print(summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je m'aime à ma sur et à ma petite\n"
     ]
    }
   ],
   "source": [
    "## LANGUAGE TRANSLATION\n",
    "\n",
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "#tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"translate English to French: I love my sister and little niece Nikole.\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7603302001953125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> supercute <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "## SENTENCE SIMILARITY\n",
    "\n",
    "stsb_sentence_1 = (\"Harrison was fighting in the war.\")\n",
    "stsb_sentence_2 = (\"Harrison's fighting style is comical.\")\n",
    "input_ids = tokenizer(\"stsb sentence 1: \"+stsb_sentence_1+\" sentence 2: \"+stsb_sentence_2, return_tensors=\"pt\").input_ids\n",
    "stsb_ids = model.generate(input_ids)\n",
    "stsb = tokenizer.decode(stsb_ids[0],skip_special_tokens=True)\n",
    "print(stsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
