{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffPYsaiza0PO"
      },
      "source": [
        "# Get started with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRETn2MsZcj2"
      },
      "source": [
        "## Table of Content\n",
        "\n",
        "- Installation\n",
        "- Set-up OpenAI Key\n",
        "- Langchain Components\n",
        "  - Model\n",
        "  - Prompt Template\n",
        "  - Output Parser\n",
        "  - Chain\n",
        "  - Memory\n",
        "  - Agents\n",
        "  - Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAlcXsnEbFcn"
      },
      "source": [
        "> Note: If you're running this notebook in google colab it's always a best practice is to change the Runtime\n",
        ">\n",
        "> Runtime -> Change Runtime type -> Select T4 GPU hardware accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGWPekniY2QT"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaOFbvD2a9lM",
        "outputId": "8b7502d3-075a-4016-d3ec-ea332587c2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.63.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.35 (from langchain)\n",
            "  Downloading langchain_core-0.3.35-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.8-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/teng/development/mlenv/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/teng/development/mlenv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/teng/development/mlenv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/teng/development/mlenv/lib/python3.10/site-packages (from langchain) (3.11.11)\n",
            "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
            "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /home/teng/development/mlenv/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from openai) (4.8.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Using cached jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /home/teng/development/mlenv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /home/teng/development/mlenv/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/teng/development/mlenv/lib/python3.10/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/teng/development/mlenv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /home/teng/development/mlenv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /home/teng/development/mlenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /home/teng/development/mlenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/teng/development/mlenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.35->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /home/teng/development/mlenv/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Using cached orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/teng/development/mlenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/teng/development/mlenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/teng/development/mlenv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/teng/development/mlenv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/teng/development/mlenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.63.2-py3-none-any.whl (472 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "Downloading langchain_core-0.3.35-py3-none-any.whl (413 kB)\n",
            "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading langsmith-0.3.8-py3-none-any.whl (332 kB)\n",
            "Downloading SQLAlchemy-2.0.38-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard, tenacity, orjson, jsonpatch, jiter, greenlet, distro, async-timeout, SQLAlchemy, requests-toolbelt, openai, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 5.0.1\n",
            "    Uninstalling async-timeout-5.0.1:\n",
            "      Successfully uninstalled async-timeout-5.0.1\n",
            "Successfully installed SQLAlchemy-2.0.38 async-timeout-4.0.3 distro-1.9.0 greenlet-3.1.1 jiter-0.8.2 jsonpatch-1.33 langchain-0.3.19 langchain-core-0.3.35 langchain-text-splitters-0.3.6 langsmith-0.3.8 openai-1.63.2 orjson-3.10.15 requests-toolbelt-1.0.0 tenacity-9.0.0 zstandard-0.23.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEAVDigY4Eq"
      },
      "source": [
        "## Set-up OpenAI Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z1lpHFGabYPm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import langchain\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://models.inference.ai.azure.com\",\n",
        "    api_key=os.environ[\"GITHUB_TOKEN\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW3EvYfXbsIR"
      },
      "outputs": [],
      "source": [
        "# openai_key = \"sk-abc\" #replace the key with your OpenAI Key\n",
        "# os.environ['OPENAI_API_KEY'] = openai_key\n",
        "# openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh99LS6nY8JG"
      },
      "source": [
        "## Model\n",
        "\n",
        "Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different. Rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loJQ6YeWm7zy"
      },
      "source": [
        "> Note: Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HswtN32MmKbV"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from langchain.chat_models import ChatOpenAI\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
            "File \u001b[0;32m~/development/mlenv/lib/python3.10/site-packages/langchain/llms/__init__.py:545\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m llms\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
          ]
        }
      ],
      "source": [
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFiBmSRcns6q"
      },
      "source": [
        "## LLMs\n",
        "\n",
        "Large Language Models (LLMs) are the models that take a text string as input, and return a text string as output.\n",
        "\n",
        "## Chat Models\n",
        "\n",
        "Chat Models are the models that are usually backed by a language model, but their APIs are more structured. Specifically, these models take a list of Chat Messages as input, and return a Chat Message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XAV0wLF_m-e3"
      },
      "outputs": [],
      "source": [
        "# To control the randomness and creativity of the generated, use temperature = 0.0 to 0.9\n",
        "chat = ChatOpenAI(temperature=0.0,max_tokens = 500)\n",
        "#chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cGHfxoOZpAy5",
        "outputId": "07e21320-0762-45b0-e44f-dd97df4fb960"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gpt-3.5-turbo'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbhGQiBvofFN",
        "outputId": "1d56cde4-baf8-4149-e629-2a2ea679bbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "print(chat.max_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W_b9n4_2ol6C"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.0)\n",
        "#llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Bm37EtGsouU9",
        "outputId": "e58e49d7-d12c-4e73-de05-04b5c0e70432"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'text-davinci-003'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueq1OZF4ozTj",
        "outputId": "99017dce-bcb0-409b-939d-9feb20ca4b86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow13c936Y-g1"
      },
      "source": [
        "## Prompt Template\n",
        "\n",
        "LangChain offers tools for crafting and utilizing prompt templates, which are pre-defined guidelines for generating prompts for language models, including instructions, few-shot examples, and context-specific questions suitable for various tasks. These templates aim to be model-agnostic, allowing for easy reuse across different language models. Generally, language models anticipate prompts to be either a string or a series of chat messages.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hrMASx4Vpllj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPiXKRlN3Pq0"
      },
      "source": [
        "> Let's create a prompt template, notice the variables inside {} curly brackets are treated as input variables and can be overriden based on the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXi6ZM0r42gY"
      },
      "source": [
        "## Please play around with your own prompts to understand how Prompt template works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4wKxvL7sptvW"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Generate a report \\\n",
        "highlighting the key findings from the {analysis_type} analysis. \\\n",
        "Here's a snippet of the analysis: \\\n",
        "```{analysis_snippet}```.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N-ynHVs7qimq"
      },
      "outputs": [],
      "source": [
        "prompt_temp = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLsV_yZ0qsvk",
        "outputId": "f86e85c2-c845-401f-e6ca-ae3249582277"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['analysis_snippet', 'analysis_type'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['analysis_snippet', 'analysis_type'], output_parser=None, partial_variables={}, template=\"Generate a report highlighting the key findings from the {analysis_type} analysis. Here's a snippet of the analysis: ```{analysis_snippet}```.\\n\", template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRLYzoXcqvNU",
        "outputId": "2c90b292-f22b-4ad5-9680-fb6d91137e02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['analysis_snippet', 'analysis_type']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_temp.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDqlS71dqzjM",
        "outputId": "8b95b8e1-14eb-4a43-85a8-bbc3fc07e5b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['analysis_snippet', 'analysis_type'], output_parser=None, partial_variables={}, template=\"Generate a report highlighting the key findings from the {analysis_type} analysis. Here's a snippet of the analysis: ```{analysis_snippet}```.\\n\", template_format='f-string', validate_template=True), additional_kwargs={})]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_temp.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vWfOUrGDq6Ge"
      },
      "outputs": [],
      "source": [
        "sample_analysis_type = \"user_engagement\"\n",
        "\n",
        "sample_analysis_snippet = \"\"\"\n",
        "Over the past quarter, the user engagement metrics have shown promising growth.\n",
        "The number of active users has increased by 15%, with a notable uptick in daily logins.\n",
        "This increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users.\n",
        "Additionally, user interactions with key features, such as commenting and sharing, have risen by 20%.\n",
        "This signifies a higher level of user participation and social engagement within our platform.\n",
        "We've also observed longer average session durations, indicating that users are spending more time exploring the content we offer.\n",
        "The engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months.\n",
        "This suggests that our efforts to improve user experience and introduce new features are resonating well with our audience.\n",
        "\n",
        "Overall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgqeTMx3jXf"
      },
      "source": [
        "> Using **format_messages** function we can override the input variables used in the Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LSLfLrwOrFZB"
      },
      "outputs": [],
      "source": [
        "response_message = prompt_temp.format_messages(\n",
        "    analysis_type =sample_analysis_type,\n",
        "    analysis_snippet = sample_analysis_snippet\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k87S2-MeRdlM",
        "outputId": "60d1d0f9-abcb-4107-c81a-c31ec07e219b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Generate a report highlighting the key findings from the user_engagement analysis. Here's a snippet of the analysis: ```\\nOver the past quarter, the user engagement metrics have shown promising growth.\\nThe number of active users has increased by 15%, with a notable uptick in daily logins.\\nThis increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users.\\nAdditionally, user interactions with key features, such as commenting and sharing, have risen by 20%.\\nThis signifies a higher level of user participation and social engagement within our platform.\\nWe've also observed longer average session durations, indicating that users are spending more time exploring the content we offer.\\nThe engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months.\\nThis suggests that our efforts to improve user experience and introduce new features are resonating well with our audience.\\n\\nOverall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\\n```.\\n\", additional_kwargs={}, example=False)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "AjGAcGrVtnZ2",
        "outputId": "5b590c85-2fed-41dd-b703-b2de0be1c76b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Generate a report highlighting the key findings from the user_engagement analysis. Here's a snippet of the analysis: ```\\nOver the past quarter, the user engagement metrics have shown promising growth.\\nThe number of active users has increased by 15%, with a notable uptick in daily logins.\\nThis increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users.\\nAdditionally, user interactions with key features, such as commenting and sharing, have risen by 20%.\\nThis signifies a higher level of user participation and social engagement within our platform.\\nWe've also observed longer average session durations, indicating that users are spending more time exploring the content we offer.\\nThe engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months.\\nThis suggests that our efforts to improve user experience and introduce new features are resonating well with our audience.\\n\\nOverall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\\n```.\\n\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_message[0].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FIIb7Hugre6L"
      },
      "outputs": [],
      "source": [
        "report = chat(response_message) #chatopenai model-GPT3.5 turbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "M3Wa3I8k1b7s"
      },
      "outputs": [],
      "source": [
        "report_llm = llm(response_message[0].content) #OpenAI -llms - Text davinci-003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C011QqOIrj_5",
        "outputId": "13ef1506-264f-4933-ad7a-e6b3626e95ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Engagement Analysis Report\n",
            "\n",
            "Key Findings:\n",
            "\n",
            "1. Active Users: The number of active users has increased by 15% over the past quarter. This growth can be attributed to the successful launch of our new mobile app, which has received positive feedback from users. The increase in active users indicates a growing user base and interest in our platform.\n",
            "\n",
            "2. Daily Logins: There has been a notable uptick in daily logins, which further supports the increase in active users. This suggests that users are finding value in our platform and are regularly accessing it to engage with the content.\n",
            "\n",
            "3. User Interactions: User interactions with key features, such as commenting and sharing, have risen by 20%. This signifies a higher level of user participation and social engagement within our platform. The increase in user interactions indicates that users are actively engaging with the content and finding it valuable enough to share and comment on.\n",
            "\n",
            "4. Average Session Durations: We have observed longer average session durations, indicating that users are spending more time exploring the content we offer. This suggests that users are finding the content engaging and are willing to spend more time on our platform.\n",
            "\n",
            "5. Engagement Rate: The engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months. This suggests that our efforts to improve user experience and introduce new features are resonating well with our audience. The increase in engagement rate indicates that users are becoming more active and engaged with the platform.\n",
            "\n",
            "Overall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. The increase in active users, daily logins, user interactions, average session durations, and engagement rate all indicate a growing user base and increased user engagement. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\n"
          ]
        }
      ],
      "source": [
        "print(report.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2bv81B92AI4",
        "outputId": "aab3b291-749b-4c2e-9344-6c434b5a3030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Report Summary:\n",
            "\n",
            "The user engagement metrics for the past quarter have shown promising growth. The number of active users has increased by 15%, with a notable uptick in daily logins. This increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users. Additionally, user interactions with key features, such as commenting and sharing, have risen by 20%. This signifies a higher level of user participation and social engagement within our platform. We've also observed longer average session durations, indicating that users are spending more time exploring the content we offer. The engagement rate has experienced a steady increase of 10% over the past two months.\n",
            "\n",
            "Overall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\n"
          ]
        }
      ],
      "source": [
        "print(report_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oockujjt2G9p"
      },
      "source": [
        "## Output in specific format and not string\n",
        "\n",
        "When developing an application, there may be a requirement for the response to adhere to a specific format, such as JSON or XML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Bdh5g6lGuZqN"
      },
      "outputs": [],
      "source": [
        "movie_review_template = \"\"\"\\\n",
        "For the following movie review, extract the following information:\n",
        "\n",
        "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
        "\n",
        "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
        "\n",
        "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
        "\n",
        "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "genre\n",
        "lead_chemistry\n",
        "plot_twists\n",
        "runtime_feel\n",
        "\n",
        "review_text: {review_text}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLYLCWWH0ZO8",
        "outputId": "43442a0e-ae10-4b1e-a886-8fea04c1f623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['review_text'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review_text'], output_parser=None, partial_variables={}, template=\"For the following movie review, extract the following information:\\n\\ngenre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\\n\\nlead_chemistry: Describe the chemistry between the lead characters, if mentioned.\\n\\nplot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\\n\\nruntime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\\n\\nFormat the output as JSON with the following keys:\\ngenre\\nlead_chemistry\\nplot_twists\\nruntime_feel\\n\\nreview_text: {review_text}\\n\", template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_bot = ChatPromptTemplate.from_template(movie_review_template)\n",
        "movie_bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1VyuG8u0ed_",
        "outputId": "6265faf9-3426-4b0d-9585-7fe53592e08a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['review_text']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_bot.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XMzdDTw60Jz5"
      },
      "outputs": [],
      "source": [
        "movie_review = \"\"\"\\\n",
        "This movie was an incredible cinematic experience. It offers a unique blend of genres:\\\n",
        "romance, drama, and thrilling action. The storyline is gripping, and the characters'\\\n",
        "emotional journey kept me engaged throughout. The visuals are stunning, especially\\\n",
        "during the breathtaking action sequences.\n",
        "\n",
        "The acting was exceptional, with the lead actors delivering performances that truly\\\n",
        "captured the essence of their roles. The chemistry between the main characters added\\\n",
        "depth to the romantic elements of the plot. The supporting cast also deserves praise,\\\n",
        "as they brought their characters to life convincingly.\n",
        "\n",
        "I was pleasantly surprised by the unexpected plot twists that kept me on the edge of\\\n",
        "my seat. The pacing was well-managed, allowing for a perfect balance between intense\\\n",
        "action and quieter, reflective moments.\n",
        "\n",
        "While the movie was a bit longer than usual, I found myself immersed in the story\\\n",
        "and didn't mind the extended runtime. It's a film that leaves a lasting impression,\\\n",
        "prompting viewers to reflect on its themes long after the credits roll.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WTxO_gVh0j0J"
      },
      "outputs": [],
      "source": [
        "delegate_review = movie_bot.format_messages(\n",
        "    review_text = movie_review\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6stCe9p0wwF",
        "outputId": "8c06e108-5b43-4e32-b995-f3ed515058d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"For the following movie review, extract the following information:\\n\\ngenre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\\n\\nlead_chemistry: Describe the chemistry between the lead characters, if mentioned.\\n\\nplot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\\n\\nruntime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\\n\\nFormat the output as JSON with the following keys:\\ngenre\\nlead_chemistry\\nplot_twists\\nruntime_feel\\n\\nreview_text: This movie was an incredible cinematic experience. It offers a unique blend of genres:romance, drama, and thrilling action. The storyline is gripping, and the characters'emotional journey kept me engaged throughout. The visuals are stunning, especiallyduring the breathtaking action sequences.\\n\\nThe acting was exceptional, with the lead actors delivering performances that trulycaptured the essence of their roles. The chemistry between the main characters addeddepth to the romantic elements of the plot. The supporting cast also deserves praise,as they brought their characters to life convincingly.\\n\\nI was pleasantly surprised by the unexpected plot twists that kept me on the edge ofmy seat. The pacing was well-managed, allowing for a perfect balance between intenseaction and quieter, reflective moments.\\n\\nWhile the movie was a bit longer than usual, I found myself immersed in the storyand didn't mind the extended runtime. It's a film that leaves a lasting impression,prompting viewers to reflect on its themes long after the credits roll.\\n\\n\", additional_kwargs={}, example=False)]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "delegate_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tK2LXfJO0vWM"
      },
      "outputs": [],
      "source": [
        "get_movie_details = chat(delegate_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7OB9tys1K85",
        "outputId": "f81c2ac3-c8f0-422e-a5f7-d5fa9112c59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"genre\": [\"romance\", \"drama\", \"thrilling action\"],\n",
            "  \"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\n",
            "  \"plot_twists\": true,\n",
            "  \"runtime_feel\": \"I found myself immersed in the story and didn't mind the extended runtime.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(get_movie_details.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGer29GG4HX3"
      },
      "source": [
        "As you can see the output is in the json format but the data type is still string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zAn8KLo1RVS",
        "outputId": "6b6c86c9-a7f1-4fa0-8f6c-7d9d204dd5c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(get_movie_details.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muDGC1K65HML"
      },
      "source": [
        "> In the code below, the .get command is used with JSON to retrieve details. However, since our response is in string format, this will result in an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "tBdiIYMh1UQa",
        "outputId": "c6b952e2-c6af-4555-d5ae-0d487bdec53a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c96fa309c60a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_movie_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genre\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'AIMessage' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "get_movie_details.get(\"genre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGTZmOuZAm2"
      },
      "source": [
        "## Output Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuHTm7xw5etb"
      },
      "source": [
        "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
        "\n",
        "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
        "\n",
        "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
        "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ajDSkkFZuHoV"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HBh73CO5zO0"
      },
      "source": [
        " StructuredOutputParser output parser can be used when you want to return multiple fields. While the Pydantic/JSON parser is more powerful, we initially experimented with data structures having text fields only.\n",
        "\n",
        " To create such multiple data structures we use ResponseSchema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Cd47oOJS0JqD"
      },
      "outputs": [],
      "source": [
        "genre_schema = ResponseSchema(name=\"genre\",\n",
        "                              description=\"Mentioned genres in the review. \\\n",
        "                              Extract and list them as a comma separated Python list.\")\n",
        "\n",
        "lead_chemistry_schema = ResponseSchema(name=\"lead_chemistry\",\n",
        "                                       description=\"Describe the chemistry between \\\n",
        "                                       the lead characters, if mentioned.\")\n",
        "\n",
        "plot_twists_schema = ResponseSchema(name=\"plot_twists\",\n",
        "                                    description=\"Were any unexpected plot twists mentioned? \\\n",
        "                                    Answer True if yes, False if not or unknown.\")\n",
        "\n",
        "runtime_feel_schema = ResponseSchema(name=\"runtime_feel\",\n",
        "                                     description=\"Did the reviewer mention their feelings about \\\n",
        "                                     the movie's runtime? If yes, extract their sentiment about it.\")\n",
        "\n",
        "response_schemas = [genre_schema,\n",
        "                    lead_chemistry_schema,\n",
        "                    plot_twists_schema,\n",
        "                    runtime_feel_schema]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MUBGGppv12eq"
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWnwhP-P16kl",
        "outputId": "b9f9bcf4-e02e-44d8-fe5a-b524987661e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"genre\": string  // Mentioned genres in the review.                               Extract and list them as a comma separated Python list.\n",
            "\t\"lead_chemistry\": string  // Describe the chemistry between                                        the lead characters, if mentioned.\n",
            "\t\"plot_twists\": string  // Were any unexpected plot twists mentioned?                                     Answer True if yes, False if not or unknown.\n",
            "\t\"runtime_feel\": string  // Did the reviewer mention their feelings about                                      the movie's runtime? If yes, extract their sentiment about it.\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFx0HtUJ58pm"
      },
      "source": [
        "> Note: Use this format_instructions in your prompt template to get the response in JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zWNf7Vdw297O"
      },
      "outputs": [],
      "source": [
        "movie_review_template = \"\"\"\\\n",
        "For the following movie review, extract the following information:\n",
        "\n",
        "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
        "\n",
        "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
        "\n",
        "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
        "\n",
        "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "genre\n",
        "lead_chemistry\n",
        "plot_twists\n",
        "runtime_feel\n",
        "\n",
        "review_text: {review_text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hkgdV8Sg3Cov"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template=movie_review_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Lo8Sc5YK3JoB"
      },
      "outputs": [],
      "source": [
        "user_review = prompt.format_messages(\n",
        "    review_text=movie_review,\n",
        "    format_instructions=format_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-SqcS5pn3cSs"
      },
      "outputs": [],
      "source": [
        "response = chat(user_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKzt8qfI3g46",
        "outputId": "5c05f7f2-239a-4f8c-8d60-8d6ffddd2d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "\t\"genre\": [\"romance\", \"drama\", \"thrilling action\"],\n",
            "\t\"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\n",
            "\t\"plot_twists\": true,\n",
            "\t\"runtime_feel\": \"While the movie was a bit longer than usual, I found myself immersed in the story and didn't mind the extended runtime.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "tFiYdTEg3oJB"
      },
      "outputs": [],
      "source": [
        "final_ans = output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWRMnN4-3t22",
        "outputId": "99342c11-0597-47bb-bbcc-bddc3cdd2acd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'genre': ['romance', 'drama', 'thrilling action'],\n",
              " 'lead_chemistry': 'The chemistry between the main characters added depth to the romantic elements of the plot.',\n",
              " 'plot_twists': True,\n",
              " 'runtime_feel': \"While the movie was a bit longer than usual, I found myself immersed in the story and didn't mind the extended runtime.\"}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIoZLfCD3vYu",
        "outputId": "970f67d7-3baf-40d3-f507-ee7e376a671f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['romance', 'drama', 'thrilling action']"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_ans.get(\"genre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZTs-eQlZCU8"
      },
      "source": [
        "## Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "v4E8lJeluDQb"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "yffDaLET4lde"
      },
      "outputs": [],
      "source": [
        "movie_review_template = \"\"\"\\\n",
        "For the following movie review, extract the following information:\n",
        "\n",
        "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
        "\n",
        "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
        "\n",
        "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
        "\n",
        "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "genre\n",
        "lead_chemistry\n",
        "plot_twists\n",
        "runtime_feel\n",
        "\n",
        "review_text: {review_text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rhaPHT4Z4pTu"
      },
      "outputs": [],
      "source": [
        "user_prompt = ChatPromptTemplate.from_template(movie_review_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "eH_xZKgr4-YQ"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0.0) #ChatOPENAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdbFtBlA5n3s"
      },
      "source": [
        "## LLM Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp23y8o56fZd"
      },
      "source": [
        "> Note:\n",
        ">\n",
        "> verbose = True, shows the logs that happens in the background\n",
        ">\n",
        "> verbose = False, displays no logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5fDdj0u37UR",
        "outputId": "fad99805-cf3b-4110-e881-08df28a1a4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: For the following movie review, extract the following information:\n",
            "\n",
            "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
            "\n",
            "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
            "\n",
            "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
            "\n",
            "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "genre\n",
            "lead_chemistry\n",
            "plot_twists\n",
            "runtime_feel\n",
            "\n",
            "review_text: This movie was an incredible cinematic experience. It offers a unique blend of genres:romance, drama, and thrilling action. The storyline is gripping, and the characters'emotional journey kept me engaged throughout. The visuals are stunning, especiallyduring the breathtaking action sequences.\n",
            "\n",
            "The acting was exceptional, with the lead actors delivering performances that trulycaptured the essence of their roles. The chemistry between the main characters addeddepth to the romantic elements of the plot. The supporting cast also deserves praise,as they brought their characters to life convincingly.\n",
            "\n",
            "I was pleasantly surprised by the unexpected plot twists that kept me on the edge ofmy seat. The pacing was well-managed, allowing for a perfect balance between intenseaction and quieter, reflective moments.\n",
            "\n",
            "While the movie was a bit longer than usual, I found myself immersed in the storyand didn't mind the extended runtime. It's a film that leaves a lasting impression,prompting viewers to reflect on its themes long after the credits roll.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "chain = LLMChain(llm=chat,prompt=user_prompt,verbose=True) # by default - Verbose=False\n",
        "review_res = chain.run(movie_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3xtk6No5QUr",
        "outputId": "7f766e0c-3e36-45ef-a5d9-be0bcc0f6d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"genre\": [\"romance\", \"drama\", \"thrilling action\"],\n",
            "  \"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\n",
            "  \"plot_twists\": true,\n",
            "  \"runtime_feel\": \"I found myself immersed in the story and didn't mind the extended runtime.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(review_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saNMWaH95t83"
      },
      "source": [
        "## SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Kl_Hz2jm7D-C"
      },
      "outputs": [],
      "source": [
        "chain_one = LLMChain(llm=chat,prompt=user_prompt,output_key = \"json_answer\") #outputkey can be any name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "M5XaWtTL43_Q"
      },
      "outputs": [],
      "source": [
        "summary_review = \"Please summarize the following review in 1 liner: {review_text}\"\n",
        "second_prompt_template = ChatPromptTemplate.from_template(summary_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Zo2u3PAv79tI"
      },
      "outputs": [],
      "source": [
        "chain_two = LLMChain(llm =chat,prompt=second_prompt_template,output_key = \"summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Xh47BGp28IND"
      },
      "outputs": [],
      "source": [
        "seq_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two],\n",
        "    input_variables=[\"review_text\"],\n",
        "    output_variables=[\"json_answer\", \"summary\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOvw3okD9NxS",
        "outputId": "7693b36b-1ee1-44d2-beb7-2ab3182f7395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "seq_response = seq_chain(movie_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Q5yBWh9mf8",
        "outputId": "149ebbc8-6d25-4818-8707-ae7a0c0cc64e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'review_text': \"This movie was an incredible cinematic experience. It offers a unique blend of genres:romance, drama, and thrilling action. The storyline is gripping, and the characters'emotional journey kept me engaged throughout. The visuals are stunning, especiallyduring the breathtaking action sequences.\\n\\nThe acting was exceptional, with the lead actors delivering performances that trulycaptured the essence of their roles. The chemistry between the main characters addeddepth to the romantic elements of the plot. The supporting cast also deserves praise,as they brought their characters to life convincingly.\\n\\nI was pleasantly surprised by the unexpected plot twists that kept me on the edge ofmy seat. The pacing was well-managed, allowing for a perfect balance between intenseaction and quieter, reflective moments.\\n\\nWhile the movie was a bit longer than usual, I found myself immersed in the storyand didn't mind the extended runtime. It's a film that leaves a lasting impression,prompting viewers to reflect on its themes long after the credits roll.\\n\",\n",
              " 'json_answer': '{\\n  \"genre\": [\"romance\", \"drama\", \"thrilling action\"],\\n  \"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\\n  \"plot_twists\": true,\\n  \"runtime_feel\": \"I found myself immersed in the story and didn\\'t mind the extended runtime.\"\\n}',\n",
              " 'summary': 'This movie is an incredible cinematic experience with a unique blend of genres, gripping storyline, stunning visuals, exceptional acting, unexpected plot twists, and a lasting impression.'}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9qCcV76ZPEN"
      },
      "source": [
        "## Memory\n",
        "\n",
        "![lanchain_memory](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "y2HfomJzuIo_"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "JPVhMaEi-094"
      },
      "outputs": [],
      "source": [
        "temp1 = ChatPromptTemplate.from_template(\"Hi, my name is {name}, I work at AI Planet\")\n",
        "temp1 = temp1.format_messages(name=\"Tarun Jain\")\n",
        "res = chat(temp1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9SNFERv_BCR",
        "outputId": "725a361a-c966-4534-8145-77e174fe4779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Tarun Jain! Nice to meet you. How can I assist you today?', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "fW5hGiebAND5"
      },
      "outputs": [],
      "source": [
        "temp2 = ChatPromptTemplate.from_template(\"As you know I work at {startup}. What is my name?\")\n",
        "temp2 = temp2.format_messages(startup=\"AI Planet\")\n",
        "res2 = chat(temp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI53HIjgAmYM",
        "outputId": "64356c57-edcd-4e4c-c6af-5f51fa9da554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but as an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. Therefore, I don't know your name unless you explicitly tell me. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AzdM39rAu7L"
      },
      "source": [
        "## Memory\n",
        "\n",
        "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "7uWdn_gHv-Ta"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "sPbo3QNv-S9r"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "JMM7ASwd-Wx1",
        "outputId": "77ebe5f6-3aad-4cc6-d142-fe14d2de9407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\""
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Tarun Jain, I work at AI Planet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "b6e91rE8-kdf",
        "outputId": "cb4c2c55-a779-4c5d-bb81-5c7258c0f5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI: Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\n",
            "Human: At AI Planet I work as a DevRel & Community Manager\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's great! As a DevRel & Community Manager, your role is to build and maintain relationships with developers and the community. You likely organize events, create content, and provide support to developers using AI technologies. It's an important role in fostering a strong developer community and driving adoption of AI solutions. Is there anything specific you would like to discuss or any questions you have about your role?\""
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"At AI Planet I work as a DevRel & Community Manager\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "l-VNHCgHAxPk",
        "outputId": "3bc0bf73-eb95-474f-bf25-00f388f2ca52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI: Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\n",
            "Human: At AI Planet I work as a DevRel & Community Manager\n",
            "AI: That's great! As a DevRel & Community Manager, your role is to build and maintain relationships with developers and the community. You likely organize events, create content, and provide support to developers using AI technologies. It's an important role in fostering a strong developer community and driving adoption of AI solutions. Is there anything specific you would like to discuss or any questions you have about your role?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your name is Tarun Jain.'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTQC4NlxA5yW",
        "outputId": "4bb005ff-da6b-4627-ba99-667aca28b59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI: Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\n",
            "Human: At AI Planet I work as a DevRel & Community Manager\n",
            "AI: That's great! As a DevRel & Community Manager, your role is to build and maintain relationships with developers and the community. You likely organize events, create content, and provide support to developers using AI technologies. It's an important role in fostering a strong developer community and driving adoption of AI solutions. Is there anything specific you would like to discuss or any questions you have about your role?\n",
            "Human: What is my name?\n",
            "AI: Your name is Tarun Jain.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71cd6L-w7EY9"
      },
      "source": [
        "## Agents\n",
        "\n",
        "Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOFHJA_Y8PeY",
        "outputId": "ce9fa3a8-740e-410f-ba5c-e756c6628cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-3.8.5-py3-none-any.whl (18 kB)\n",
            "Collecting aiofiles>=23.1.0 (from duckduckgo-search)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Requirement already satisfied: lxml>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (4.9.3)\n",
            "Collecting httpx[brotli,http2,socks]>=0.24.1 (from duckduckgo-search)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (1.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting brotli (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (3.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (1.1.3)\n",
            "Installing collected packages: brotli, socksio, hyperframe, hpack, h11, aiofiles, httpcore, h2, httpx, duckduckgo-search\n",
            "Successfully installed aiofiles-23.2.1 brotli-1.1.0 duckduckgo-search-3.8.5 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.17.3 httpx-0.24.1 hyperframe-6.0.1 socksio-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BiEsn8BQ7fEi"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "dMh-UNFW70vd"
      },
      "outputs": [],
      "source": [
        "search = DuckDuckGoSearchAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search Engine\",\n",
        "        func=search.run,\n",
        "        description=\"To explore the world of internet\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "HV5fgIew8UAJ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "llm = ChatOpenAI(openai_api_key=openai_key, temperature=0.0)\n",
        "agent_chain = initialize_agent(tools, llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D00FnXls8jVs"
      },
      "source": [
        "### Agent Type: Do refer the official documentation [docs](https://python.langchain.com/docs/modules/agents/agent_types/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "pl5F00fx8tVv",
        "outputId": "6be1b97a-bd9b-45c2-b0cf-354e38335303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Search Engine\",\n",
            "    \"action_input\": \"Location of Bangalore\"\n",
            "}\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIt lies 3,113 feet (949 metres) above sea level, atop an east-west ridge in the Karnataka Plateau in the southeastern part of the state, at a cultural meeting point of the Kannada -, Telugu -, and Tamil -speaking peoples. Pop. (2001) city, 4,301,326; urban agglom., 5,701,446; (2011) city, 8,443,675; urban agglom., 8,520,435. History MapsofIndia.com - Map showing the location of Bangalore,Karnataka in India. Find where is Bangalore located. Bengaluru Railway Map. Bengaluru or Bangalore is the capital of Karnataka and one of the most prominent cities of India. It has a population of 8,443,675 (2011 census) and ranks as the third ... About Map: Map showing location of Bangalore in the state of Karnatak, India Where is Bengaluru Located? Bangalore officially Bengaluru is the capital and largest city of the Indian state of Karnataka. Bengaluru lies between 12°58′44″ North latitudes and 77°35′30° East longitudes. The city covers an area of about 741 km2 with a ... Bangaluru Location Map, Karnataka. Click here for Customized Maps. Print. Email. Free Download. Buy Now. * Map showing the location of Bangalore in Karnataka. Disclaimer: All efforts have been ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Bangalore is located in the southeastern part of the state of Karnataka, India. It is situated atop an east-west ridge in the Karnataka Plateau, at an elevation of 3,113 feet (949 meters) above sea level. Bangalore is a cultural meeting point for the Kannada, Telugu, and Tamil-speaking peoples.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bangalore is located in the southeastern part of the state of Karnataka, India. It is situated atop an east-west ridge in the Karnataka Plateau, at an elevation of 3,113 feet (949 meters) above sea level. Bangalore is a cultural meeting point for the Kannada, Telugu, and Tamil-speaking peoples.'"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.run(input=\"Where is Bangalore located?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otjsw1wc7Iex"
      },
      "source": [
        "## Reference:\n",
        "\n",
        "- [AIWithTarun Video](https://www.youtube.com/watch?v=IbMtSXTJ0ic)\n",
        "- [Langchain documentation](https://docs.langchain.com/docs/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
